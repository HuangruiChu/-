{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHR2+1D Model train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#基本的引入\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from tensorboardX import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms as T\n",
    "import matplotlib.pyplot as plt # plt 用于显示图片\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 设置随机数种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "# 设置随机数种子\n",
    "setup_seed(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform设定："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = T.Compose([   \n",
    "        T.Resize([224, 224]),\n",
    "        #T.RandomRotation(degrees=5, expand=True),\n",
    "       # T.CenterCrop([32,32]),\n",
    "        #T.RandomHorizontalFlip(p=0.5),\n",
    "       # T.RandomVerticalFlip(p=0.5),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean= [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "\n",
    "transform_test = T.Compose([\n",
    "        T.Resize([224, 224]),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean= [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义MyDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算提取视频哪几帧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \n",
    "    \n",
    "    def __init__(self, file_path=None,data=\"train\",transform=None):   \n",
    "        \"\"\"\n",
    "        初始化自定义Dataset类的参数\n",
    "        Attributes\n",
    "            file_path: 字符串，数据集的存储路径，例如‘./UCF101/train’ 或 './UCF101/eval'等\n",
    "            all_people:P0001-P0250演员\n",
    "            classes  : 列表，每个元素为一个字符串，代表一个子类别，例如['dog', 'airplane', ...]等\n",
    "            transform: 传入一个从torchvision.transforms定义的数据预处理\n",
    "        \"\"\"\n",
    "        self.data=data\n",
    "        self.sample_length= 8\n",
    "        self.all_people=os.listdir(file_path)\n",
    "        self.all_people.remove(\".ipynb_checkpoints\")#删除多于的文件夹\n",
    "        self.classes = os.listdir(os.path.join(file_path,self.all_people[0]))\n",
    "        self.cur_classes=[\"A\",\"G\",\"F\",\"D\",\"B\"]\n",
    "        #class这边要重新写，我只看一些classes\n",
    "        self.transform = transform\n",
    "        # 初始化给定文件夹下的所有数据\n",
    "        self.init_all_data(file_path) \n",
    "\n",
    "        return None\n",
    "        \n",
    "\n",
    "    def init_all_data(self, file_path):\n",
    "        \"\"\"\n",
    "        初始化该数据集内所有的图像及其对应的标签，保存在self.videos和self.labels两个列表内\n",
    "        Attributes\n",
    "            file_path: 字符串，数据集文件夹的存储路径\n",
    "        \"\"\"\n",
    "        # 初始化两个列表，记录该数据集内每一张图片的完整路径及其对应的标签\n",
    "        self.videos = []\n",
    "        self.labels = []\n",
    "        # 遍历所有的子类别，并得到每个子类别对应的文件夹路径\n",
    "        total_num=0\n",
    "        for people in self.all_people:\n",
    "            total_num+=1\n",
    "            if self.data==\"train\":\n",
    "                if total_num<=2:\n",
    "                    #当前人的所有数据\n",
    "                    for idx, cls in enumerate(self.cur_classes):\n",
    "                         #现在只训练5个classes\n",
    "                        for level in range(8):\n",
    "                            cls_path = os.path.join(file_path,people,cls,str(level))\n",
    "                            cams=\"cam_1\"#现在我只取cam_1的数据\n",
    "                            cur_video = os.path.join(cls_path, cams,\"color.avi\")\n",
    "                            if self.is_valid_video(cur_video ):\n",
    "                                self.videos.append(cur_video )\n",
    "                                self.labels.append(idx)\n",
    "                else:\n",
    "                    break\n",
    "            elif self.data==\"test\":\n",
    "                if total_num>248:\n",
    "                    #当前人的所有数据\n",
    "                    for idx, cls in enumerate(self.cur_classes):\n",
    "                          #现在只训练5个classes\n",
    "                        for level in range(8):\n",
    "                            cls_path = os.path.join(file_path,people,cls,str(level))\n",
    "                            cams=\"cam_1\"#现在我只取cam_1的数据\n",
    "                            cur_video = os.path.join(cls_path, cams,\"color.avi\")\n",
    "                            if self.is_valid_video(cur_video ):\n",
    "                                self.videos.append(cur_video )\n",
    "                                self.labels.append(idx)\n",
    "        return None\n",
    "\n",
    "        \n",
    "    def is_valid_video(self, video_path):\n",
    "        \"\"\"\n",
    "        判断图片是否为可以打开的有效文件\n",
    "        Attributes\n",
    "            img_path: 字符串，待检测图片的存储路径\n",
    "        Returns\n",
    "            valid: 布尔变量，True/False分别表示该图片是否可以正常打开\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 若读取成功，设valid为True\n",
    "            vc=cv2.VideoCapture(video_path)\n",
    "            valid=vc.isOpened()   \n",
    "        except:\n",
    "            # 若读取失败，设valid为False\n",
    "            valid = False\n",
    "            \n",
    "        return valid\n",
    "        \n",
    "\n",
    "    \n",
    "   \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        按给定索引，获取对应的视频及其标签\n",
    "        Attributes\n",
    "            idx: int类型数字，表示目标图像的索引\n",
    "        Returns\n",
    "            frames: 一个打开的PIL.Image对象，是PIL库存储图像的一种数据格式（类似于OpenCV利用numpy张量存储图像）\n",
    "            label: int类型，表示对应的类别，例如假设self.classes=['cat', 'dog', 'airplane']，则label=1代表‘dog'类别；\n",
    "                   对于pytorch的分类，不需要特意将其变成onehot向量，因为crossentropy函数内置了这部分功能。\n",
    "        \"\"\"\n",
    "        # 利用PIL.Image.open打开图片，并将其强制转化为RGB格式（防止数据集中混杂灰度图，导致读取出单通道图片，送入网络因矩阵维度不一致而报错）\n",
    "        \n",
    "        start = time.time()\n",
    "        frames=[]#储存所有的frames\n",
    "        cur_video=self.videos[idx]\n",
    "        vc = cv2.VideoCapture(cur_video) #读入视频文件\n",
    "        video_len = int(vc.get(cv2.CAP_PROP_FRAME_COUNT))  # 视频总帧数\n",
    "        gap=video_len//16#计算gap\n",
    "        rval=vc.isOpened()      #判断视频是否打开  返回True或Flase\n",
    "        c = 1\n",
    "        while rval and len(frames)<16:  # 读取视频帧当帧数满16就停止\n",
    "            rval, frame = vc.read()  # videoCapture.read() 函数，第一个返回值为是否成功获取视频帧，第二个返回值为返回的视频帧：\n",
    "            if rval:\n",
    "#                 start = time.time()\n",
    "                if (c%gap==0): # 如果c在我要读取的帧的集合里，就把它储存起来\n",
    "                    frame=Image.fromarray(cv2.cvtColor(frame,cv2.COLOR_BGR2RGB))#转成PIL\n",
    "                    frames.append(frame)\n",
    "#                     end1=time.time()\n",
    "#                     print(\"the time to get one frame is \")\n",
    "#                     print(end1-start)\n",
    "                c = c + 1\n",
    "            else:\n",
    "                break\n",
    "        vc.release()  \n",
    "        \n",
    "        \n",
    "        # 进行预处理的变换\n",
    "        #tensor 化\n",
    "        ts=time.time()\n",
    "        frames=torch.stack(\n",
    "                    [self.transform(c) for c in frames]\n",
    "                )\n",
    "        # [S, T, H, W, C] -> [S, C, T, H, W]\n",
    "        frames=frames.permute(1,0,2,3)#相位变化\n",
    "        # 获取对应的标签\n",
    "        label = self.labels[idx]\n",
    "        one_hot=np.zeros(5)\n",
    "        one_hot[label]=1\n",
    "        end = time.time()\n",
    "#         print(\"我花在get item上的时间 \")\n",
    "#         print (end-start)\n",
    "        return frames, one_hot\n",
    "   \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        获取数据集中图像的总数，该方法的作用是用于DataLoader去调用，从而获取在给定Batch Size的情况下，一个Epoch的总长，\n",
    "        从而可以在一个Epoch结束时实现shuffle数据集的功能\n",
    "        \"\"\"\n",
    "        return len(self.videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 核心model\n",
    "\n",
    "### 借用他人训练好的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import r2plus1d_34_32_ig65m,r2plus1d_34_32_kinetics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "MODELS = {\n",
    "    # Model name followed by the number of output classes.\n",
    "    \"r2plus1d_34_32_ig65m\": 359,\n",
    "    \"r2plus1d_34_32_kinetics\": 400,\n",
    "    \"r2plus1d_34_8_ig65m\": 487,\n",
    "    \"r2plus1d_34_8_kinetics\": 400,\n",
    "}\n",
    "def init_model(\n",
    "    sample_length: int, base_model: str, num_classes: int = None\n",
    ") -> torchvision.models.video.resnet.VideoResNet:\n",
    "    \"\"\"\n",
    "    Initializes the model by loading it using torch's `hub.load`\n",
    "    functionality. Uses the model from TORCH_R2PLUS1D.\n",
    "\n",
    "    Args:\n",
    "        sample_length: Number of consecutive frames to sample from a video (i.e. clip length).\n",
    "        base_model: the R2plus1D model is based on either ig65m or kinetics.\n",
    "        num_classes: the number of classes/actions\n",
    "\n",
    "    Returns:\n",
    "        Load a model from a github repo, with pretrained weights\n",
    "    \"\"\"\n",
    "    if base_model not in (\"ig65m\", \"kinetics\"):\n",
    "        raise ValueError(\n",
    "            f\"Not supported model {base_model}. Should be 'ig65m' or 'kinetics'\"\n",
    "        )\n",
    "\n",
    "    # Decide if to use pre-trained weights for DNN trained using 8 or for 32 frames\n",
    "    model_name = f\"r2plus1d_34_{sample_length}_{base_model}\"\n",
    "\n",
    "    print(f\"Loading {model_name} model\")\n",
    "    ##这里之后要稍微加一些数据\n",
    "    if model_name==\"r2plus1d_34_32_ig65m\":\n",
    "        model= r2plus1d_34_32_ig65m(num_classes=MODELS[model_name], pretrained=True)\n",
    "    else:\n",
    "        model=r2plus1d_34_32_kinetics(num_classes=MODELS[model_name], pretrained=True)\n",
    "\n",
    "    # Replace head\n",
    "    if num_classes is not None:\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "    return model, model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#只用valid_loss\n",
    "def train_model(model,device, n_epochs):\n",
    "    \n",
    "    # to track the training loss as the model trains\n",
    "    train_losses = []\n",
    "    train_loss_min=100\n",
    "    train_correct_labels=[0,0,0,0,0]#后面可以改的\n",
    "    train_actual_labels=[0,0,0,0,0]\n",
    "    # to track the average training loss per epoch as the model trains\n",
    "    avg_train_losses = []\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    " \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train() # prep model for training\n",
    "        correct_labels=[0,0,0,0,0]\n",
    "        actual_labels=[0,0,0,0,0]\n",
    "        predict_labels=[0,0,0,0,0]\n",
    "        for step, (X, y) in enumerate(tqdm(train_loader)):\n",
    "            X, y = X.to(device), y.type(torch.FloatTensor).to(device)  #,dtype=torch.int64\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(X)\n",
    "            # calculate the loss\n",
    "            loss = loss_func(output, y)\n",
    "            #计算acc\n",
    "            final_out=output.cpu().detach().numpy()\n",
    "            labels=y.cpu()\n",
    "            for i in range(len(labels)):\n",
    "                actual_labels+=labels[i].numpy()\n",
    "                predict_label=np.argmax(final_out[i])\n",
    "                predict_labels[predict_label]+=1\n",
    "#                 print(labels[i])\n",
    "#                 print(predict_label)\n",
    "#                 print(labels[i][predict_label])\n",
    "                if labels[i][predict_label]==1:\n",
    "#                     print(\"Correct!\")\n",
    "                    correct_labels[predict_label]+=1\n",
    "                    train_correct_labels[predict_label]+=1\n",
    "            train_actual_labels+=actual_labels\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # record training loss\n",
    "            train_losses.append(loss.item())\n",
    "        # print training/validation statistics \n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = np.average(train_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        cur_train_acc=np.average(np.array(correct_labels)/np.array(actual_labels))\n",
    "       \n",
    "        \n",
    "        epoch_len = len(str(n_epochs))\n",
    "        \n",
    "        print_msg = (f'[{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +\n",
    "                     f'train_loss: {train_loss:.5f} '\n",
    "                   + f'train_acc: {cur_train_acc:.5f}'\n",
    "                    )\n",
    "        \n",
    "        print(print_msg)\n",
    "        print(\"the outcome of the predicted lable for this epoch\")\n",
    "        print(predict_labels)\n",
    "        print(\"the outcome of the correct lable for this epoch\")\n",
    "        print(correct_labels)\n",
    "        \n",
    "        if train_loss<train_loss_min:\n",
    "            torch.save(model.state_dict(), 'checkpoint.pt')\t# 这里会存储迄今最优模型的参数\n",
    "            train_loss_min = train_loss\n",
    "        # clear lists to track next epoch\n",
    "        train_losses = []\n",
    "#     # load the last checkpoint with the best model\n",
    "    print(\"the final correct prediction of each class\")\n",
    "    print(train_correct_labels)\n",
    "    print(\"Done!\")\n",
    "    train_acc=np.array(train_correct_labels)/np.array(train_actual_labels)\n",
    "    return  model, avg_train_losses,train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设立model和device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading r2plus1d_34_32_ig65m model\n"
     ]
    }
   ],
   "source": [
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# create a new model with these weights\n",
    "model, model_name= init_model( num_classes=5,sample_length=32,base_model=\"ig65m\")\n",
    "#model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "optimizer=torch.optim.SGD(model.parameters(), lr=0.001, momentum=0, dampening=0, weight_decay=0, nesterov=False)\n",
    "# optimizer=torch.optim.Adam(model.parameters(),lr=lr,weight_decay=1e-5)\n",
    "loss_func = nn.BCELoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设立dataset和dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MyDataset(\"KONE1-250\",\n",
    "                      data=\"train\",\n",
    "                       transform=transform_train)\n",
    "# test_data = MyDataset(\"KONE1-250\",\n",
    "#                       data=\"test\",\n",
    "#                        transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Num_workers=0\n",
    "Batch_size=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=DataLoader(dataset=train_data,batch_size=Batch_size,\n",
    "                             shuffle=False, num_workers=Num_workers)\n",
    "# test_loader=DataLoader(dataset=test_data,batch_size=Batch_size,\n",
    "#                              shuffle=False, num_workers=Num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "# for X,y in test_loader:\n",
    "#     X, y = X.to(device), y.type(torch.FloatTensor).to(device) \n",
    "#     labels=y.cpu().tolist()\n",
    "#     for i in labels[0]:\n",
    "#         print(type(i))\n",
    "#     output = model(X)\n",
    "#     print(output.cpu().detach().numpy())\n",
    "# #     print(\"sepereate of loss\")\n",
    "# #     for i in range(len(y[0])):\n",
    "# #         print(loss_func(output[0][i], y[0][i]))\n",
    "# #     print(\"total loss\")\n",
    "#     loss = loss_func(output, y)\n",
    "#     print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:56<00:00,  2.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/50] train_loss: 0.65117 train_acc: 0.20000\n",
      "the outcome of the predicted lable for this epoch\n",
      "[80, 0, 0, 0, 0]\n",
      "the outcome of the correct lable for this epoch\n",
      "[16, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:56<00:00,  2.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2/50] train_loss: 0.59068 train_acc: 0.20000\n",
      "the outcome of the predicted lable for this epoch\n",
      "[80, 0, 0, 0, 0]\n",
      "the outcome of the correct lable for this epoch\n",
      "[16, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:55<00:00,  2.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3/50] train_loss: 0.55288 train_acc: 0.20000\n",
      "the outcome of the predicted lable for this epoch\n",
      "[80, 0, 0, 0, 0]\n",
      "the outcome of the correct lable for this epoch\n",
      "[16, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:56<00:00,  2.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4/50] train_loss: 0.52984 train_acc: 0.20000\n",
      "the outcome of the predicted lable for this epoch\n",
      "[80, 0, 0, 0, 0]\n",
      "the outcome of the correct lable for this epoch\n",
      "[16, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:56<00:00,  2.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5/50] train_loss: 0.51582 train_acc: 0.18750\n",
      "the outcome of the predicted lable for this epoch\n",
      "[79, 0, 0, 1, 0]\n",
      "the outcome of the correct lable for this epoch\n",
      "[15, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:56<00:00,  2.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6/50] train_loss: 0.50706 train_acc: 0.20000\n",
      "the outcome of the predicted lable for this epoch\n",
      "[71, 0, 0, 9, 0]\n",
      "the outcome of the correct lable for this epoch\n",
      "[14, 0, 0, 2, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 4/40 [00:15<02:15,  3.76s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-7258185492ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#optimizer=torch.optim.Adam(model.parameters(),lr=lr,weight_decay=1e-4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-799e2d57695d>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, device, n_epochs)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;31m# record training loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;31m# print training/validation statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# calculate average loss over an epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs=50\n",
    "#optimizer=torch.optim.Adam(model.parameters(),lr=lr,weight_decay=1e-4)\n",
    "model, train_loss,train_acc= train_model(model ,device, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the loss as the network trained\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.plot(range(1,len(train_loss)+1),train_loss, label='Training Loss')\n",
    "plt.plot(range(1,len(valid_loss)+1),valid_loss,label='Validation Loss')\n",
    "plt.xlabel('epochs',fontsize=30)\n",
    "plt.ylabel('loss',fontsize=30)\n",
    "\n",
    "# find position of lowest validation loss\n",
    "minposs = valid_loss.index(min(valid_loss))+1 \n",
    "plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
    "\n",
    "#plt.ylim(0, 0.5) # consistent scale\n",
    "plt.xlim(1, len(train_loss)+1) # consistent scale\n",
    "plt.yticks(fontsize=30)\n",
    "plt.xticks(fontsize=30)\n",
    "plt.grid(True)\n",
    "plt.legend(loc = 'best',fontsize=30)\n",
    "plt.tight_layout()\n",
    "fig.savefig('Unet_100_loss.png', bbox_inches='tight',dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
